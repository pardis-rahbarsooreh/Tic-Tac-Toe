{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Exercise: Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README\n",
    "We strongly recommend you install `gymnasium>=0.28.1` and `python>=3.10.*`. You can set up a python environment using e.g. [conda](https://docs.conda.io/projects/miniconda/en/latest/) in the terminal\n",
    "```\n",
    "conda create -n rl23 python=3.10 gymnasium=0.28.1 notebook -c conda-forge\n",
    "conda activate rl23\n",
    "```\n",
    "The `notebook` package is required for opening and working with this jupyter notebook.\n",
    "In a terminal with the Python environment active, run\n",
    "```\n",
    "jupyter notebook ExerciseD.ipynb\n",
    "```\n",
    "\n",
    "If you need help setting up a development environment, please visit the office hour or ask for help in the forum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Imports\n",
    "\n",
    "from typing import Any, Dict, Callable\n",
    "from dataclasses import dataclass\n",
    "from functools import cached_property, partial\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "**Solve the tasks D1 and D2 below and submit your final `.ipynb` file as a solution to the mCMS.**\n",
    "\n",
    "During grading, we will clear outputs and then run all cells.\n",
    "Notebooks that produce runtime errors will be graded with 0 points, unless the errors arise due to gymnasium/numpy backwards compatibility issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "The task of the programming exercise is to program an agent that learns to play Tic-Tac-Toe against different opponents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opponent Policies\n",
    "\n",
    "We load and initialize the opponent policies of increasing strength from .json-files. They are stored in global variables and can therefore be easily changed at any point of the notebook. The opponent policies are of increasing strength.\n",
    "\n",
    "For loading the policies, the folder `Opponent_Policies` containing the .json-files of the policies should lie in the same directory as this notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load opponent policy from .json-file. \n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "opponent_policy_file = Path('Opponent_Policies') # Change filename to play against different policy.\n",
    "\n",
    "with open(opponent_policy_file / 'policy1.json') as json_file:\n",
    "    opponent_policy_1 = json.load(json_file)\n",
    "\n",
    "with open(opponent_policy_file / 'policy2.json') as json_file:\n",
    "    opponent_policy_2 = json.load(json_file)\n",
    "\n",
    "with open(opponent_policy_file / 'policy3.json') as json_file:\n",
    "    opponent_policy_3 = json.load(json_file)\n",
    "\n",
    "with open(opponent_policy_file / 'policy4.json') as json_file:\n",
    "    opponent_policy_4 = json.load(json_file)\n",
    "\n",
    "# Set opponent policy\n",
    "opponent_policy_dict = opponent_policy_1 # Change to play against different opponent policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gymnasium Environment for Tic-Tac-Toe\n",
    "\n",
    "We implement a Gymnasium environment simulating a game of Tic-Tac-Toe. We thereby use\n",
    "- as possible field values  $V = \\{0,1,2\\}$, whereby $v = 0$ stands for a 'O'-field, $v = 1$ for an empty field, and $v = 2$ for a 'X'-field.\n",
    "- as state space $S = V^{3 \\times 3}$. A state `s` is stored as a `list[list[int]]`, `s[i][j]` refers then to the value in the i-th row in the j-th column.\n",
    "- action space $A = V \\times V = \\{(0,0),(0,1),(0,2),(1,0),(1,1),(1,2), (2,0),(2,1),(2,2)\\}$.\n",
    "\n",
    "### Environment Dynamics\n",
    "\n",
    "We implement Tic-Tac-Toe as a sequential decision problem. The agent plays against a specified opponent policy (see above). One step of the environment looks as follows: \n",
    "1. Perform the move of the agent. The agent marks fields with 'X'.\n",
    "2. Check whether this has finished the game, i.e. win for the agent or draw. If the game is finished, terminate episode and compute the reward. \n",
    "3. Perform the game of the opponent. The opponent marks fields with 'O'.\n",
    "4. Check whether this has finished the game, i.e. win for the opponent or draw. If the game is finished, terminate episode and compute the reward. \n",
    "\n",
    "#### Initial state\n",
    "We randomize whether the agent or the opponent starts with the first move. Hence, the initial state of the sequential decision problem is either\n",
    "- a completely empty field, for the case that the agent has the first move, or\n",
    "- a field with one '0', for the case that the opponent has the first move. \n",
    "\n",
    "\n",
    "#### Rewards\n",
    "Rewards are only gained when the game is finished: \n",
    "- Reward of 1, if the agent wins. \n",
    "- Reward of 0, if the game ends in a draw. \n",
    "- Reward of -1, if the opponent wins. \n",
    "\n",
    "#### Executable Actions \n",
    "Notice that not all actions are always executable: If a field `s[i][j]` is non-empty, then the action $(i,j)$ is not executable. If the agent tries to perform a non-executable action, the environment raises an Exception. Hence, make sure that the agent only picks executable actions (the opponent policy chooses only executable actions as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some preliminary and auxiliary definitions \n",
    "\n",
    "# Definitions of possible field values\n",
    "CROSS, EMPTY, CIRCLE = 2, 1, 0  \n",
    "\n",
    "def get_rows(state: list[list[int]]) -> [list[list[int]], list[list[int]], list[list[int]]]:\n",
    "    \"\"\"\n",
    "    Helper function: Returns list of rows, list of columns, and list of diagonals\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Compute rows\n",
    "    rows = state\n",
    "\n",
    "    # Compute columns\n",
    "    columns = []\n",
    "    for j in range(3):\n",
    "        column = []\n",
    "        for i in range(3):\n",
    "            column.append(state[i][j])\n",
    "        columns.append(column)\n",
    "    \n",
    "    #Compute diagonals\n",
    "    diagonal0 = []\n",
    "    diagonal1 = []\n",
    "    for i in range(3):\n",
    "        diagonal0.append(state[i][i])\n",
    "        diagonal1.append(state[2-i][i])\n",
    "    \n",
    "    # Return rows, columns, and diagonals. \n",
    "    return rows, columns, [diagonal0, diagonal1]\n",
    "\n",
    "# Gymnasium environment for Tic-Tac-Toe\n",
    "class SysadminEnv(gym.Env):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.action_space = spaces.MultiDiscrete([3,3]) # Action space \n",
    "        self.observation_space = spaces.MultiDiscrete([[3,3,3],[3,3,3],[3,3,3]]) # State space\n",
    "        self.reset_counter = 0\n",
    "\n",
    "\n",
    "    @property\n",
    "    def get_reset_counter(self):\n",
    "        return self.reset_counter\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def occupied_fields(self) -> int | None:\n",
    "        \"\"\"\n",
    "        Returns the number of occupied fields.\n",
    "\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"_state\"):\n",
    "            return None\n",
    "        \n",
    "        res = 0\n",
    "        for l in self._state:\n",
    "            for v in l:\n",
    "                if v != EMPTY:\n",
    "                    res = res + 1\n",
    "            \n",
    "        return res\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def game_finished(self) -> int | None:\n",
    "        \"\"\"\n",
    "        Returns None if game is not finished.\n",
    "\n",
    "        Returns 0 if circle wins.\n",
    "        Returns 1 if it is a draw.\n",
    "        Returns 2 if crosses wins. \n",
    "\n",
    "        \"\"\"\n",
    "        rows, columns, diagonals = get_rows(self._state)\n",
    "\n",
    "        for l in rows + columns + diagonals:\n",
    "            if all(v == CROSS for v in l):\n",
    "                return 2\n",
    "            if all(v == CIRCLE for v in l):\n",
    "                return 0\n",
    "            \n",
    "        if self.occupied_fields == 9:\n",
    "            return 1\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    \n",
    "    def opponent_policy (self) -> [int,int]:\n",
    "        \"\"\"\n",
    "            Takes random action from the list of moves of the opponent policy.\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self, \"_state\"):\n",
    "            raise Exception(\"Unable to find opponent move in uninitialized environment.\")\n",
    "        \n",
    "        opponent_action_list = opponent_policy_dict[self._state.__str__()]\n",
    "        return opponent_action_list[np.random.choice(len(opponent_action_list))]\n",
    "\n",
    "    \n",
    "    def perform_move(self, move: [int, int], cross: bool):\n",
    "        \"\"\"\n",
    "        Returns the number of occupied fields.\n",
    "\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"_state\"):\n",
    "            raise Exception(\"Unable to perform move in uninitialized environment.\")\n",
    "\n",
    "        if self._state[move[0]][move[1]] != EMPTY:\n",
    "            raise Exception(\"Unable to perform move on occupied field.\")\n",
    "        \n",
    "        if cross: \n",
    "            self._state[move[0]][move[1]] = 2\n",
    "        else:\n",
    "            self._state[move[0]][move[1]] = 0\n",
    "\n",
    "        \n",
    "    def reset(\n",
    "        self, *, seed: int | None = None, options: dict[str, Any] | None = None\n",
    "    ) -> tuple[np.ndarray, dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Resets the environment to its initial state.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # increment reset_counter\n",
    "        self.reset_counter += 1\n",
    "\n",
    "        # All fields are empty initially\n",
    "        self._state = [[EMPTY,EMPTY,EMPTY],[EMPTY,EMPTY,EMPTY],[EMPTY,EMPTY,EMPTY]]\n",
    "\n",
    "        # Random choice whether agent or opponent makes the first move. \n",
    "        # In case of opponent, first move of opponent is performed.\n",
    "        if np.random.random() < 0.5:   \n",
    "            self.perform_move(self.opponent_policy(), False)\n",
    "        \n",
    "        return self._state, dict()\n",
    "   \n",
    "\n",
    "    def step(self, action: [int,int]) -> tuple[np.ndarray, float, bool, bool, dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Performs a step in the environment given an action of the agent.\n",
    "\n",
    "        Return: new_state, reward, done, truncated, information_dictionary (last two return values are irrelevant for our purposes)  \n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Perform agent's move\n",
    "        self.perform_move(action, True)\n",
    "\n",
    "        # Check whether game is finished and compute the return\n",
    "        finished = self.game_finished  \n",
    "        if self.game_finished == 2:\n",
    "            return self._state, 1, True, False, dict()\n",
    "        if self.game_finished == 1:\n",
    "            return self._state, 0, True, False, dict()\n",
    "\n",
    "\n",
    "        # Perform opponent's move\n",
    "        self.perform_move(self.opponent_policy(), False)\n",
    "\n",
    "        # Check whether game is finished and compute the return\n",
    "        finished = self.game_finished  \n",
    "        if finished is None:\n",
    "            return self._state, 0, False, False, dict()\n",
    "        elif finished == 0:\n",
    "            return self._state, -1, True, False, dict()\n",
    "        elif finished == 1:\n",
    "            return self._state, 0, True, False, dict()    \n",
    "\n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"\n",
    "        Prints the current state of the field to the command line. \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self, \"_state\"):\n",
    "           raise Exception(\"Unable to visualize uninitialized environment.\")\n",
    "       \n",
    "        res = [[\"\",\"\",\"\"],[\"\",\"\",\"\"],[\"\",\"\",\"\"]] \n",
    "\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                v = self._state[i][j]\n",
    "                if v == CROSS:\n",
    "                    res[i][j] = \"X\"\n",
    "                elif v == EMPTY:\n",
    "                    res[i][j] = \" \"\n",
    "                elif v == CIRCLE:\n",
    "                    res[i][j] = \"O\"\n",
    "                else: \n",
    "                    raise Exception(\"Invalid value in TicTacToe Field\")\n",
    "\n",
    "        for l in res: \n",
    "            print(l)\n",
    "    \n",
    "        print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register environment\n",
    "gym.register(\"Sysadmin-ED\", partial(SysadminEnv))\n",
    "env = gym.make(\"Sysadmin-ED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example on how to experiment with the environment. \n",
    "\n",
    "env.reset(seed=42)\n",
    "\n",
    "print(env.step((1,2)))\n",
    "env.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise D.1 (2 Points)\n",
    "\n",
    "Briefly describe the learning algorithm you have implemented. When using function approximation, describe in particular the features that you are using. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise D.2 (2+2+2+2+4* Points)\n",
    "\n",
    "Implement a learning algorithm that learns to play Tic-Tac-Toe. For function approximation learning, the function `RL_algorithm` should return the learned feature weights.\n",
    "\n",
    "In the case that you are not using function approximation but a different RL algorithm, the arguments and return types of the functions below are allowed to be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given hyperparameter gamma\n",
    "gamma = 1\n",
    "\n",
    "# TODO If neeeded, add further hyperparameter that are used by the implemented algorithm. \n",
    "\n",
    "\n",
    "def agent_policy(state, weights):\n",
    "    \"\"\"\n",
    "    Policy of the agent: Given the environment state and feature weights, returns the best estimated performable action. \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #TODO\n",
    "    \n",
    "    raise NotImplementedError\n",
    "    \n",
    "\n",
    "def training_algorithm(num_episodes: int):\n",
    "    \"\"\"\n",
    "    Reinforcement learning algorithm: For function approximation learning, learn the feature weights for the given number of training episodes, i.e. env.reset() is allowed to be called num_episodes many times. \n",
    "    \n",
    "    Returns the learned feature weights. \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #TODO\n",
    "        \n",
    "    raise NotImplementedError\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation \n",
    "\n",
    "We evaluate the learned polices multiple times against the different opponent policies using the script below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(weights, eval_episodes: int):\n",
    "    \"\"\"\n",
    "    Evaluates the agent's policy described by the learned weights by simulating the given number of episodes. \n",
    "    Returns the overall number of wins, draws, looses, and the statistical mean of the episode returns.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    returns = []\n",
    "    wins, draws, looses = 0,0,0\n",
    "\n",
    "    for episode in range(eval_episodes):\n",
    "        \n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            action = agent_policy(state, weights)\n",
    "            _, reward, done, _,_ = env.step(action)\n",
    "            if done: \n",
    "                if reward == 1: \n",
    "                    wins = wins + 1\n",
    "                elif reward == 0:\n",
    "                    draws = draws + 1\n",
    "                elif reward == -1:\n",
    "                    looses = looses + 1\n",
    "\n",
    "                returns.append(reward)  \n",
    "          \n",
    "    return wins, draws, looses, np.mean(returns)\n",
    "\n",
    "\n",
    "opponent_policy_dict = opponent_policy_1 # Change to play against different opponent policy.\n",
    "\n",
    "# Policy testing\n",
    "training_episodes = 5000 # Number of training episodes\n",
    "test_episodes = 100 # Number of test episodes\n",
    "test_runs = 5 # Number of test runs\n",
    "\n",
    "for i in range (test_runs):\n",
    "    env = gym.make(\"Sysadmin-ED\") \n",
    "    weights = training_algorithm(training_episodes) # learn the weights via function approximation learning\n",
    "    \n",
    "    # Check that number of episodes is not exceeded\n",
    "    if env.get_reset_counter > training_episodes:\n",
    "            raise RuntimeError(f\"Exceeded maximal number of calls of reset function\")\n",
    "    \n",
    "    wins, draws, looses, average_return = evaluate_policy(weights, test_episodes) # evaluate the learned policy\n",
    "    print(f\"Training iteration {i}: Wins: {wins}, Draws: {draws}, Looses: {looses}, Average Return: {average_return}\") # print results of the current test run\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
